--- tcp_bbr.c	2021-08-12 16:15:06.530999944 +0800
+++ tcp_bbr.c.new	2021-08-12 21:31:44.102277010 +0800
@@ -86,8 +86,11 @@
 /* BBR congestion control block */
 struct bbr {
 	u32	min_rtt_us;	        /* min RTT in min_rtt_win_sec window */
+	u32	rtt_us;	        /* min RTT in min_rtt_win_sec window */
 	u32	min_rtt_stamp;	        /* timestamp of min_rtt_us */
 	u32	probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
+	u32     probe_bw_steady_ms;
+	u32     probe_bw_steady_stamp;
 	struct minmax bw;	/* Max recent delivery rate in pkts/uS << 24 */
 	u32	rtt_cnt;	    /* count of packet-timed rounds elapsed */
 	u32     next_rtt_delivered; /* scb->tx.delivered at end of round */
@@ -125,7 +128,7 @@
 		unused_c:6;
 };
 
-#define CYCLE_LEN	8	/* number of phases in a pacing gain cycle */
+#define CYCLE_LEN	3	/* number of phases in a pacing gain cycle */
 
 /* Window length of bw filter (in rounds): */
 static const int bbr_bw_rtts = CYCLE_LEN + 2;
@@ -136,6 +139,8 @@
 /* Skip TSO below the following bandwidth (bits/sec): */
 static const int bbr_min_tso_rate = 1200000;
 
+static const u32 bbr_probe_bw_steady_ms = 120;
+
 /* Pace at ~1% below estimated bw, on average, to reduce queue at bottleneck.
  * In order to help drive the network toward lower queues and low latency while
  * maintaining high utilization, the average pacing rate aims to be slightly
@@ -160,11 +165,10 @@
 static const int bbr_pacing_gain[] = {
 	BBR_UNIT * 5 / 4,	/* probe for more available bw */
 	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
-};
+	BBR_UNIT,		/* cruise at 1.0*bw to utilize pipe, without */
+};				/* creating excess queue... */
 /* Randomize the starting gain cycling phase over N phases: */
-static const u32 bbr_cycle_rand = 7;
+static const u32 bbr_cycle_rand = CYCLE_LEN - 1;
 
 /* Try to keep at least this many packets in flight, if things go smoothly. For
  * smooth functioning, a sliding window protocol ACKing every other packet
@@ -549,20 +553,32 @@
 
 /* End cycle phase if it's time and/or we hit the phase's in-flight target. */
 static bool bbr_is_next_cycle_phase(struct sock *sk,
-				    const struct rate_sample *rs)
+				    const struct rate_sample *rs,
+				    bool *upprobe)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 	bool is_full_length =
 		tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp) >
 		bbr->min_rtt_us;
+	bool filter_expired;
+	bool can_probe;
 	u32 inflight, bw;
 
+	*upprobe = false;
 	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
 	 * use the pipe without increasing the queue.
 	 */
-	if (bbr->pacing_gain == BBR_UNIT)
-		return is_full_length;		/* just use wall clock time */
+	if (bbr->pacing_gain == BBR_UNIT) {
+		filter_expired = after(tcp_jiffies32,
+			       bbr->probe_bw_steady_stamp + 
+			       msecs_to_jiffies( bbr->probe_bw_steady_ms));
+		can_probe = rs->rtt_us >= 0 && rs->rtt_us <= bbr->min_rtt_us;
+		if (can_probe)
+			*upprobe = true;
+		
+		return filter_expired || can_probe;
+	}
 
 	inflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);
 	bw = bbr_max_bw(sk);
@@ -585,13 +601,15 @@
 		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
 }
 
-static void bbr_advance_cycle_phase(struct sock *sk)
+static void bbr_advance_cycle_phase(struct sock *sk, bool upprobe)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 
-	bbr->cycle_idx = (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);
+	bbr->cycle_idx = upprobe ? 0 : (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);
 	bbr->cycle_mstamp = tp->delivered_mstamp;
+	if (bbr->cycle_idx == CYCLE_LEN - 1)
+		bbr->probe_bw_steady_stamp = tcp_jiffies32;
 }
 
 /* Gain cycling: cycle pacing gain to converge to fair share of available bw. */
@@ -599,9 +617,10 @@
 				   const struct rate_sample *rs)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
+	bool upprobe = false;
 
-	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
-		bbr_advance_cycle_phase(sk);
+	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs, &upprobe))
+		bbr_advance_cycle_phase(sk, upprobe);
 }
 
 static void bbr_reset_startup_mode(struct sock *sk)
@@ -617,7 +636,9 @@
 
 	bbr->mode = BBR_PROBE_BW;
 	bbr->cycle_idx = CYCLE_LEN - 1 - prandom_u32_max(bbr_cycle_rand);
-	bbr_advance_cycle_phase(sk);	/* flip to next phase of gain cycle */
+	if (bbr->cycle_idx == CYCLE_LEN - 1)
+		bbr->probe_bw_steady_stamp = tcp_jiffies32;
+	bbr_advance_cycle_phase(sk, false);	/* flip to next phase of gain cycle */
 }
 
 static void bbr_reset_mode(struct sock *sk)
@@ -793,8 +814,10 @@
 	 * phase when app writes faster than the network can deliver :)
 	 */
 	if (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {
+		u32 win = 4 + div64_long(bbr_probe_bw_steady_ms*1000, 
+					     bbr->min_rtt_us);
 		/* Incorporate new sample into our max bw filter. */
-		minmax_running_max(&bbr->bw, bbr_bw_rtts, bbr->rtt_cnt, bw);
+		minmax_running_max(&bbr->bw, win, bbr->rtt_cnt, bw);
 	}
 }
 
@@ -944,6 +967,7 @@
 	/* Track min RTT seen in the min_rtt_win_sec filter window: */
 	filter_expired = after(tcp_jiffies32,
 			       bbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);
+	bbr->rtt_us = rs->rtt_us;
 	if (rs->rtt_us >= 0 &&
 	    (rs->rtt_us < bbr->min_rtt_us ||
 	     (filter_expired && !rs->is_ack_delayed))) {
@@ -981,6 +1005,23 @@
 		bbr->idle_restart = 0;
 }
 
+static u32 get_pacing_gain(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u64 queuing_delay = bbr->rtt_us - bbr->min_rtt_us;
+	u32 gain = bbr_pacing_gain[bbr->cycle_idx];
+
+	if (bbr->cycle_idx == 0) {
+		gain *= queuing_delay;
+		gain = (u32)div64_long(gain, bbr->min_rtt_us);
+	} else if (bbr->cycle_idx == 1) {
+		gain *= bbr->min_rtt_us;
+		gain = (u32)div64_long(gain, queuing_delay);
+	}
+
+	return gain;
+}
+
 static void bbr_update_gains(struct sock *sk)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
@@ -997,7 +1038,7 @@
 	case BBR_PROBE_BW:
 		bbr->pacing_gain = (bbr->lt_use_bw ?
 				    BBR_UNIT :
-				    bbr_pacing_gain[bbr->cycle_idx]);
+				    get_pacing_gain(sk));
 		bbr->cwnd_gain	 = bbr_cwnd_gain;
 		break;
 	case BBR_PROBE_RTT:
